{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error # pour importer la fonction de la MAE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_pickle(\"Création du Dataset/dataset.pkl\")\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "#suppression des KDA car bcp de 0\n",
    "roles = [\"TOP\",\"JGL\",\"MID\",\"ADC\",\"SUP\"]\n",
    "colomnes = []\n",
    "for i in [0,1]:\n",
    "    for role in roles :\n",
    "        colomnes.append(f\"{role}{i}_KDA\")\n",
    "# print(colomnes)\n",
    "data = data.drop(columns = colomnes)\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "# suppression des listes pour le KDAG par transformation en plusieurs colomnes\n",
    "colomnes_KDAG=[]\n",
    "for colomne in colomnes:\n",
    "    colomnes_KDAG.append(colomne + \"G\")\n",
    "# print(colomnes_KDAG)\n",
    "\n",
    "for colomne in colomnes_KDAG:\n",
    "    nom = colomne[:-4]\n",
    "    data2 = pd.DataFrame(data[f\"{colomne}\"].to_list(),data.index,columns=[f\"{nom}K\",f\"{nom}D\",f\"{nom}A\",])\n",
    "    data[f'{nom}KDA_Ratio'] = (data2[f'{nom}K'] + data2[f'{nom}A']) / (data2[f'{nom}D']+1)\n",
    "    data = data.drop(columns = colomne)\n",
    "# print(data)\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "#champion par leur ID :\n",
    "dict_champ = {}\n",
    "L=['Aatrox', 'Ahri', 'Akali', 'Akshan', 'Alistar', 'Amumu', 'Anivia', 'Annie', 'Aphelios', 'Ashe', 'AurelionSol', 'Azir', 'Bard', 'Belveth', 'Blitzcrank', 'Brand', 'Braum', 'Caitlyn', 'Camille', 'Cassiopeia', 'Chogath', 'Corki', 'Darius', 'Diana', 'Draven', 'DrMundo', 'Ekko', 'Elise', 'Evelynn', 'Ezreal', 'FiddleSticks', 'Fiora', 'Fizz', 'Galio', 'Gangplank', 'Garen', 'Gnar', 'Gragas', 'Graves', 'Gwen', 'Hecarim', 'Heimerdinger', 'Illaoi', 'Irelia', 'Ivern', 'Janna', 'JarvanIV', 'Jax', 'Jayce', 'Jhin', 'Jinx', 'Kaisa', 'Kalista', 'Karma', 'Karthus', 'Kassadin', 'Katarina', 'Kayle', 'Kayn', 'Kennen', 'Khazix', 'Kindred', 'Kled', 'KogMaw','KSante', 'Leblanc', 'LeeSin', 'Leona', 'Lillia', 'Lissandra', 'Lucian', 'Lulu', 'Lux', 'Malphite', 'Malzahar', 'Maokai', 'MasterYi', 'MissFortune', 'MonkeyKing', 'Mordekaiser', 'Morgana', 'Nami', 'Nasus', 'Nautilus', 'Neeko', 'Nidalee', 'Nilah', 'Nocturne', 'Nunu', 'Olaf', 'Orianna', 'Ornn', 'Pantheon', 'Poppy', 'Pyke', 'Qiyana', 'Quinn', 'Rakan', 'Rammus', 'RekSai', 'Rell', 'Renata', 'Renekton', 'Rengar', 'Riven', 'Rumble', 'Ryze', 'Samira', 'Sejuani', 'Senna', 'Seraphine', 'Sett', 'Shaco', 'Shen', 'Shyvana', 'Singed', 'Sion', 'Sivir', 'Skarner', 'Sona', 'Soraka', 'Swain', 'Sylas', 'Syndra', 'TahmKench', 'Taliyah', 'Talon', 'Taric', 'Teemo', 'Thresh', 'Tristana', 'Trundle', 'Tryndamere', 'TwistedFate', 'Twitch', 'Udyr', 'Urgot', 'Varus', 'Vayne', 'Veigar', 'Velkoz', 'Vex', 'Vi', 'Viego', 'Viktor', 'Vladimir', 'Volibear', 'Warwick', 'Xayah', 'Xerath', 'XinZhao', 'Yasuo', 'Yone', 'Yorick', 'Yuumi', 'Zac', 'Zed', 'Zeri', 'Ziggs', 'Zilean', 'Zoe', 'Zyra']\n",
    "for i in range(len(L)):\n",
    "    dict_champ[L[i]]=i\n",
    "\n",
    "def ID_champ(L):\n",
    "    return(dict_champ[L])\n",
    "\n",
    "for colomne in colomnes:\n",
    "    nom = colomne[:-3]\n",
    "    data[f\"{nom}CHAMP\"] = data[f\"{nom}CHAMP\"].apply(ID_champ)\n",
    "\n",
    "# On s'occupe du rank (voir Paul) :\n",
    "def elo(L):\n",
    "    if L[0]==\"IRON\":\n",
    "        e = 0\n",
    "    elif L[0]==\"BRONZE\":\n",
    "        e = 400\n",
    "    elif L[0]==\"SILVER\":\n",
    "        e = 800\n",
    "    elif L[0]==\"GOLD\":\n",
    "        e = 1200\n",
    "    elif L[0]==\"PLATINUM\":\n",
    "        e = 1600\n",
    "    elif L[0]==\"DIAMOND\":\n",
    "        e = 2000\n",
    "    else:\n",
    "        e = 2300\n",
    "    if L[1] == 'III':\n",
    "        e += 100\n",
    "    elif L[1] == 'II':\n",
    "        e += 200\n",
    "    elif L[1] == 'I':\n",
    "        e += 300\n",
    "    return e + L[2]\n",
    "\n",
    "for c in data.columns:\n",
    "    if c[-4:] == \"RANK\":\n",
    "        data[c] = data[c].apply(elo)\n",
    "\n",
    "\n",
    "# Pour l'ensemble des rôles, ajout d'un ratio sur le LVL entre les joueurs de l'équipe 1 et 2 correspondants\n",
    "for role in roles:\n",
    "    data[f\"{role}_LVL_RATIO\"]=data[f\"{role}0_LVL\"]/data[f\"{role}1_LVL\"]\n",
    "data[\"LVL_RATIO_MEAN\"]=(data[\"SUP0_LVL\"]+data[\"ADC0_LVL\"]+data[\"MID0_LVL\"]+data[\"JGL0_LVL\"]+data[\"TOP0_LVL\"])/(data[\"SUP1_LVL\"]+data[\"ADC1_LVL\"]+data[\"MID1_LVL\"]+data[\"JGL1_LVL\"]+data[\"TOP1_LVL\"])\n",
    "\n",
    "# Ajout d'un ratio de win streak : si +4 : 4 joueurs de plus en winstreak dans l'équipe 1 que dans l'équipe 2. Prend des valeurs entre -5 et 5\n",
    "data[\"RATIO_WINSTREAK\"]=data[\"SUP0_HOT\"]*1+data[\"ADC0_HOT\"]*1+data[\"MID0_HOT\"]*1+data[\"JGL0_HOT\"]*1+data[\"TOP0_HOT\"]*1-(data[\"SUP1_HOT\"]*1+data[\"ADC1_HOT\"]*1+data[\"MID1_HOT\"]*1+data[\"JGL1_HOT\"]*1+data[\"TOP1_HOT\"]*1)\n",
    "\n",
    "#graphe rapide pour voir la répartition des victoires pour l'équipe 1 en fonction du ratio de winstreak\n",
    "df2 = data.groupby(['RATIO_WINSTREAK','Y'])['Y'].count().reset_index(name=\"nb_victoires\") # on récupère les ratios pour les analyser\n",
    "df3 = df2[df2.Y==True]\n",
    "# plt.bar(df3.RATIO_WINSTREAK,df3.nb_victoires)\n",
    "# plt.show()\n",
    "#---------------------------------------------- MI Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de calcul des MI scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Y\"].astype(int) # pour avoir des 0 et des 1 au lieu de True et False\n",
    "X=data.drop(columns = [\"Y\"])\n",
    "scores = []\n",
    "N = 10\n",
    "for i in range(N):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    clf = RandomForestClassifier(n_estimators=600,n_jobs=6)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test,y_test))\n",
    "print(f\"Moyenne des scores sur {N} modèles: {sum(scores)/len(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphe de l'importance des features dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf = RandomForestClassifier(n_estimators=600,n_jobs=6)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importance = clf.feature_importances_\n",
    "\n",
    "f_importance = 100.0 * (importance / importance.max())\n",
    "sorted_idx = np.argsort(f_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.barh(pos, f_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, [X_train.columns[u] for u in sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "def pourcentage_reussite(X,y):\n",
    "    Z = np.where(X== y,1,0)\n",
    "    return(Z.sum()/(Z.shape[0])*100)\n",
    "\n",
    "y = data[\"Y\"].astype(int) # pour avoir des 0 et des 1 au lieu de True et False\n",
    "X=data.drop(columns = [\"Y\"])\n",
    "scores = []\n",
    "N = 7\n",
    "max= (0,0,0)\n",
    "for number_of_trees in range(100,1100,100): #recherche de la meilleure profondeur\n",
    "    for learning_r in range(1,12): #recherche du meilleur learning rate\n",
    "        for i in range(N):\n",
    "            train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2)\n",
    "            modele = XGBClassifier(n_estimators = number_of_trees, learning_rate=learning_r/20, n_jobs=6)\n",
    "            modele.fit(train_X, train_y, \n",
    "                    early_stopping_rounds=5, #si l'erreur se détériore sur 5 cycles on arrête le programme même si < n_estimators\n",
    "                    eval_set=[(val_X, val_y)], #obligatoire quand early_stopping_rounds utilisé\n",
    "                    verbose=False)\n",
    "            donnees_predites = modele.predict(val_X)\n",
    "            scores.append(pourcentage_reussite(donnees_predites,val_y))\n",
    "        moyenne = sum(scores)/len(scores)\n",
    "        if moyenne>max[0]:\n",
    "            max=(moyenne,number_of_trees,learning_r/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentative de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "X=data.loc[:, [\"TOP0_LVL\",\"TOP1_LVL\"]]\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "sns.relplot(x=\"TOP0_LVL\", y=\"TOP1_LVL\", hue=\"Cluster\", data=X, height=6)\n",
    "plt.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "X=data.loc[:, [\"TOP0_GWR\",\"TOP1_GWR\"]]\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "sns.relplot(x=\"TOP0_GWR\", y=\"TOP1_GWR\", hue=\"Cluster\", data=X, height=6)\n",
    "plt.show()\n",
    "print(make_mi_scores(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Projet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e22c97e4ed8d431fc724019dffa39fd8054b2d28cf3e66fed132b173246b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
